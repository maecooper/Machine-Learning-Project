---
title: 'Machine Learning Project: Final'
author: "P. M. Cooper"
---

```{r setup, include=FALSE, cache=TRUE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE)
#download training and test sets
TrLink<- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
TsLink<- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

training<- read.csv(TrLink)
test<- read.csv(TsLink)

#download libraries and set seed for reproducibility
library(caret)
library(tidyr)
library(dplyr)
library(data.table)

set.seed(5673)
```

## Introduction

The goal of this project was to use a machine learning algorithm to predict how a person performed an exercise, based on data from accelerometers on the belt, forearm, arm, and dumbell. Using a random forest, I was able to create a good classification algorithm.  
  
## Data 
The data provided was already divided into a training and testing set, and I began by downloading both. Initial looks at the data showed that many of the variables were only available for certain observations (where new_window="yes") and that none of the test data contained information from these variables. Therefore, I removed those variables from all data sets. I also removed the variable for the test subject, as including it in the algorithm would make it less than useful outside of this particular testing environment. I also did not think that the raw time-stamp values would be useful variables, as they are mostly meaningful in relation to each other. Therefore, I also created a new variable that indicated the time since the last observation, with 0 values for the first observation for a particular test subject or window number.  
  
Additionally, although it was labeled as a test set, the 20 cases in the test set are to function as a validation set. Therefore, I also divided the training data into a true training set and a test set, with a 80/20 ratio.

```{r cache=TRUE, warning = FALSE}
#remove summary variables
train_deet<-training[,c(1:4,7:11,37:49,60:68,84:86,102,113:124,140,151:160)]
#add time since last measurement
train_deet$time_per<- with(train_deet,ifelse(user_name==shift(user_name) 
                          & num_window==shift(num_window) 
                          & !X==1,
                        shift(raw_timestamp_part_1)*1000000+shift(raw_timestamp_part_2)-(raw_timestamp_part_1*1000000+raw_timestamp_part_2),0))
#partition for testing error
intrain <- createDataPartition(y=train_deet$classe, p=.8, list=FALSE)
train_2<- train_deet[intrain,]
test_2<- train_deet[-intrain,]
```
## Building an Algorithm
I attempted two different algorithm building methods on the partitioned training set. The first was random forest, and the second was linear discriminant analysis. I did both on the same data after normalizing all of my variables. While I considered including both results, random forest had such a greater in-sample accuracy that I just used that result.  

```{r cache=TRUE, warning = FALSE}
mod_rf<-train(classe~.,data=train_2[,-(1:5)],preProcess=c("center","scale"),method="rf")
mod_lda <-train(classe~.,data=train_2[,-(1:5)],preProcess=c("center","scale"),method="lda")

#get in-sample accuracies of the 2 models
pred_rf<-predict(mod_rf,train_2)
pred_lda<-predict(mod_lda,train_2)

match_rf<-as.logical(pred_rf==train_2$classe)
match_lda<-as.logical(pred_lda==train_2$classe)

acc_rf<-sum(match_rf)/length(match_rf)*100
acc_lda<-sum(match_lda)/length(match_lda)*100
```
   
The in-sample accuracy of the random forest model was `r acc_rf`, in comparison with `r acc_lda` for LDA.  
  
Using the data from the training set that I set aside, I also checked the out-of-sample accuracy for both models.  
```{r cache=TRUE, warning = FALSE}
pred_rf2<-predict(mod_rf,test_2)
pred_lda2<-predict(mod_lda,test_2)

match_rf2<-as.logical(pred_rf2==test_2$classe)
match_lda2<-as.logical(pred_lda2==test_2$classe)

acc_rf2<-sum(match_rf2)/length(match_rf2)*100
acc_lda2<-sum(match_lda2)/length(match_lda2)*100
```
The out-of-sample accuracy of the random forest model was very high, at `r acc_rf2`, in comparison with th lower `r acc_lda2` for LDA.   
  
This can also be seen in the table of results, comparing the true classification in the test sample to the predicted classification.  
```{r cache=TRUE, warning = FALSE}
table(pred_rf2,test_2$classe,dnn=c("Predicted by RF","True Classification"))
table(pred_lda2,test_2$classe,dnn=c("Predicted by LDA","True Classification"))
```
  
## Cross-Validation
Random forest, by its very nature, consists of running many models (trees) and cross validating them, so k-fold or other type of cross validation was unnecessary on top of that. The small test sample that I took out to check that my strong results from random forest were not a fluke or result of over-fitting.   
  
## Prediction
Finally, I can use the random forest model that I built to predict values in my test (validation) set of 20 cases.

```{r cache=TRUE, warning = FALSE}
test_deet<-test[,c(1:4,7:11,37:49,60:68,84:86,102,113:124,140,151:160)]
#add time since last measurement
test_deet$time_per<- with(test_deet,ifelse(user_name==shift(user_name) 
                                               & num_window==shift(num_window) 
                                               & !X==1,
                                               shift(raw_timestamp_part_1)*1000000+shift(raw_timestamp_part_2)-(raw_timestamp_part_1*1000000+raw_timestamp_part_2),0))

final_rf_pred <-predict(mod_rf,test_deet[,-(1:5)]) 
print(final_rf_pred)
```